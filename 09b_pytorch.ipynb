{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music machine learning - Pytorch\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)\n",
    "\n",
    "In this course we will cover\n",
    "1. A [quick introduction](#intro) to Pytorch \n",
    "2. An implementation for [advanced models](#models) implementation\n",
    "3. A quick proposal for [attention](#attention) layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pytorch\n",
    "\n",
    "`Pytorch` is a Python-based scientific computing package targeted at deep learning, which provides a very large flexibility and easeness of use for GPU calculation. `Pytorch` is constructed around the concept of `Tensor`, which is very similar to `numpy.ndarray`, but can be seamlessly run on GPU.\n",
    "\n",
    "Here are some examples of different `Tensor` creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "tensor([30.2500,  9.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Create a 5 x 3 Tensor of zeros\n",
    "x = torch.empty(5, 3)\n",
    "# Create a 64 x 3 x 32 x 32 random Tensor\n",
    "x = torch.rand(64, 3, 32, 32)\n",
    "# Create a Tensor of zeros with _long_ type\n",
    "x = torch.zeros(10, 10, dtype=torch.long)\n",
    "# Construct a Tensor from the data\n",
    "x = torch.tensor([5.5, 3])\n",
    "\n",
    "# Out of place: new object in memory\n",
    "x.multiply(x)\n",
    "print(x)\n",
    "# In place: same object in memory\n",
    "x.multiply_(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or create a tensor based on an existing tensor. These methods\n",
    "will reuse properties of the input tensor, e.g. dtype, unless\n",
    "new values are provided by user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(8, 2, dtype=torch.double)      # new_* methods take in sizes\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x.size())\n",
    "print(x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arithmetic operations\n",
    "\n",
    "Tensors provide access to a transparent library of arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7358, 1.2263],\n",
      "        [1.9655, 0.7943],\n",
      "        [0.8314, 1.6781],\n",
      "        [0.2836, 0.5414],\n",
      "        [0.6348, 1.1347],\n",
      "        [1.1475, 1.2132],\n",
      "        [1.5797, 0.8383],\n",
      "        [1.1781, 1.1831]])\n",
      "tensor([[0.7358, 1.2263],\n",
      "        [1.9655, 0.7943],\n",
      "        [0.8314, 1.6781],\n",
      "        [0.2836, 0.5414],\n",
      "        [0.6348, 1.1347],\n",
      "        [1.1475, 1.2132],\n",
      "        [1.5797, 0.8383],\n",
      "        [1.1781, 1.1831]])\n",
      "tensor([[0.7358, 1.2263],\n",
      "        [1.9655, 0.7943],\n",
      "        [0.8314, 1.6781],\n",
      "        [0.2836, 0.5414],\n",
      "        [0.6348, 1.1347],\n",
      "        [1.1475, 1.2132],\n",
      "        [1.5797, 0.8383],\n",
      "        [1.1781, 1.1831]])\n",
      "tensor([[0.9463, 2.0123],\n",
      "        [2.9383, 1.0160],\n",
      "        [0.9407, 2.6222],\n",
      "        [0.4406, 0.7572],\n",
      "        [0.9967, 1.5921],\n",
      "        [1.3957, 1.9957],\n",
      "        [2.3704, 1.2015],\n",
      "        [1.9065, 1.6370]])\n",
      "tensor([[0.1548, 0.9639],\n",
      "        [1.9120, 0.1761],\n",
      "        [0.0909, 1.5843],\n",
      "        [0.0445, 0.1168],\n",
      "        [0.2297, 0.5190],\n",
      "        [0.2848, 0.9494],\n",
      "        [1.2491, 0.3045],\n",
      "        [0.8581, 0.5370]])\n",
      "tensor([[0.9674, 1.2016, 1.0454, 0.9664],\n",
      "        [1.6896, 1.5714, 1.9578, 1.0476],\n",
      "        [1.1985, 1.5508, 1.2796, 1.2727],\n",
      "        [0.3976, 0.5085, 0.4261, 0.4150],\n",
      "        [0.8622, 1.0873, 0.9278, 0.8811],\n",
      "        [1.2566, 1.4122, 1.3951, 1.0749],\n",
      "        [1.4300, 1.3949, 1.6407, 0.9655],\n",
      "        [1.2676, 1.4086, 1.4113, 1.0649]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4467/1664322904.py:12: UserWarning: An output with one or more elements was resized since it had shape [5, 3], which does not match the required output shape [8, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.add(x, y, out=result)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(8, 2)\n",
    "y = torch.rand(8, 2)\n",
    "z = torch.rand(2, 4)\n",
    "# Equivalent additions\n",
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "# Add in place\n",
    "x.add_(y)\n",
    "print(x)\n",
    "# Put in target Tensor\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "# Element_wise multiplication\n",
    "print(x * y)\n",
    "# Matrix product\n",
    "print(x @ z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing and resizing\n",
    "\n",
    "You can slice tensors using the usual Python operators. For resizing and reshaping tensor, you can use ``torch.view`` or ``torch.reshape``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2263, 0.7943, 1.6781, 0.5414, 1.1347, 1.2132, 0.8383, 1.1831])\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one element tensor, use ``.item()`` to get the value as a\n",
    "Python number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4470])\n",
      "-1.4470454454421997\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have more than 100 operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, which are all described at [https://pytorch.org/docs/torch](https://pytorch.org/docs/torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy bridge\n",
    "\n",
    "Converting a Torch Tensor to a Numpy array and vice versa is extremely simple. Note that the Pytorch Tensor and Numpy array **will share their underlying memory locations** (if the Tensor is on CPU), and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going GPU\n",
    "\n",
    "Tensors can be moved onto any device using the ``.to`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graphs\n",
    "\n",
    "The concept of a computation graph is essential to efficient deep learning programming, because it allows you to not have to write the back propagation gradients yourself. A computation graph is simply a specification of how your data is combined to give you the output (the forward pass). Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. \n",
    "\n",
    "The fundamental flag ``requires_grad`` allows to specify which variables are going to need differentiation in all these operations. If ``requires_grad=True``, the Tensor object keeps track of how it was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7fdf5fc41c30>\n"
     ]
    }
   ],
   "source": [
    "# Tensor factory methods have a ``requires_grad`` flag\n",
    "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
    "# With requires_grad=True, we can still do all the operations \n",
    "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "# But z now knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, `z` knows that it is the direct result of an addition. Furthermore, if we keep following z.grad_fn, we can even find back both `x` and `y`. But how does that help us compute a gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x7fdf099cbd00>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, what is the derivative of this sum with respect to the first\n",
    "component of x? In math, we want\n",
    "\n",
    "\\begin{align}\\frac{\\partial s}{\\partial x_0}\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "Well, s knows that it was created as a sum of the tensor z. z knows\n",
    "that it was the sum x + y. So\n",
    "\n",
    "\\begin{align}s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\end{align}\n",
    "\n",
    "And so s contains enough information to determine that the derivative we want is 1. We can have Pytorch compute the gradient, and see that we were right:\n",
    "\n",
    "**Note** : If you run this block multiple times, the gradient will increment. That is because Pytorch *accumulates* the gradient into the .grad property, since for many models this is very convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what is going on in the block below is crucial for being a\n",
    "successful programmer in deep learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "None\n",
      "<AddBackward0 object at 0x7fdf099ca260>\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(z.grad_fn)\n",
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)\n",
    "# Now z has the computation history, which we can **detach**\n",
    "new_z = z.detach()\n",
    "# Which means that we have no gradient attached anymore\n",
    "print(new_z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tensors\n",
    "with ``.requires_grad=True`` by wrapping the code block in\n",
    "``with torch.no_grad():``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining networks \n",
    "\n",
    "Here, we briefly recall that in `PyTorch`, the `nn` package provides higher-level abstractions over raw computational graphs that are useful for building neural networks. The `nn` package defines a set of `Modules`, which are roughly equivalent to neural network layers. A `Module` receives input `Tensors` and computes output `Tensors`, but may also hold internal state such as `Tensors` containing learnable parameters. In the following example, we use the `nn` package to show how easy it is to instantiate a three-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define the input dimensions\n",
    "in_size = 1000\n",
    "# Number of neurons in a layer\n",
    "hidden_size = 100\n",
    "# Output (target) dimension\n",
    "output_size = 10\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(in_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_size, output_size),\n",
    "    nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the slides, we can as easily mix between pre-defined modules and arithmetic operations. Here, we will define a *residual* block, and then combine them in a more complex network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_res=32):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim_res, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(dim_res, dim, 1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "model = nn.Sequential(\n",
    "\tResBlock(64, 32),\n",
    "\tResBlock(64, 32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our own layers\n",
    "\n",
    "In the following, we re-implement the *attention* layer, which is the basis of the infamous `Transformer` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(ChordLevelAttention, self).__init__()\n",
    "        self.mlp = nn.Linear(n_hidden, n_hidden)\n",
    "        self.u_w = nn.Parameter(torch.rand(n_hidden))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # get the hidden representation of the sequence\n",
    "        u_it = F.tanh(self.mlp(X))\n",
    "        # get attention weights for each timestep\n",
    "        alpha = F.softmax(torch.matmul(u_it, self.u_w), dim=1)\n",
    "        # get the weighted sum of the sequence\n",
    "        out = torch.sum(torch.matmul(alpha, X), dim=1)\n",
    "        return out, alpha\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
